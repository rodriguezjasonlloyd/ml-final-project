{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4a897dfe-bcb8-4bc3-9ba4-b25c1de016d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass, field, replace, asdict\n",
    "from typing import List, Optional\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "@dataclass\n",
    "class DataConfig:\n",
    "    data_dir: str = 'data'\n",
    "    img_size: int = 224\n",
    "    batch_size: int = 32\n",
    "    validation_split: float = 0.2\n",
    "    seed: int = 123\n",
    "\n",
    "@dataclass\n",
    "class ModelConfig:\n",
    "    backbone: str = 'EfficientNetB0'\n",
    "    pretrained: bool = True\n",
    "    unfreeze_blocks: int = 2\n",
    "    blocks_to_unfreeze: Optional[List[int]] = None\n",
    "\n",
    "@dataclass\n",
    "class TrainingConfig:\n",
    "    epochs: int = 10\n",
    "    learning_rate: float = 1e-4\n",
    "    optimizer: str = 'adam'\n",
    "    loss: str = 'binary_crossentropy'\n",
    "    metrics: List[str] = field(default_factory=lambda: ['accuracy'])\n",
    "\n",
    "@dataclass\n",
    "class CallbackConfig:\n",
    "    tensorboard_logdir: str = 'logs/'\n",
    "    checkpoint_dir: str = 'checkpoints/'\n",
    "\n",
    "@dataclass\n",
    "class ExperimentConfig:\n",
    "    data: DataConfig = field(default_factory=DataConfig)\n",
    "    model: ModelConfig = field(default_factory=ModelConfig)\n",
    "    training: TrainingConfig = field(default_factory=TrainingConfig)\n",
    "    callbacks: CallbackConfig = field(default_factory=CallbackConfig)\n",
    "\n",
    "\n",
    "def print_block(title: str, data: dict):\n",
    "    labels = {k: (f\"{v:.2f}\" if isinstance(v, float) else str(v)) for k, v in data.items()}\n",
    "    lbl_w = max(len(k) for k in labels)\n",
    "    val_w = max(len(v) for v in labels.values())\n",
    "    total_w = lbl_w + 2 + val_w + 2\n",
    "    title_str = f\" {title} \"\n",
    "    border = \"=\" * max(len(title_str), total_w)\n",
    "    print(f\"\\n{border}\")\n",
    "    print(title_str.center(len(border)))\n",
    "    print(border)\n",
    "    for k, v in labels.items():\n",
    "        print(f\"{k.ljust(lbl_w)} : {v.rjust(val_w)}\")\n",
    "    print(border + \"\\n\")\n",
    "\n",
    "\n",
    "def summarize_model(base):\n",
    "    block_layers = {}\n",
    "    block_trainable = {}\n",
    "    for layer in base.layers:\n",
    "        m = re.match(r\"^block(\\d+)[a-z]?_\", layer.name)\n",
    "        blk = int(m.group(1)) if m else 0\n",
    "        block_layers[blk] = block_layers.get(blk, 0) + 1\n",
    "        block_trainable[blk] = block_trainable.get(blk, False) or layer.trainable\n",
    "    rows = []\n",
    "    for blk in sorted(block_layers):\n",
    "        name = f\"block{blk}\" if blk > 0 else \"stem/head\"\n",
    "        rows.append((name, str(block_layers[blk]), 'Yes' if block_trainable[blk] else 'No'))\n",
    "    headers = ('Block', 'Layers', 'Trainable')\n",
    "    col1 = max(len(r[0]) for r in rows + [headers])\n",
    "    col2 = max(len(r[1]) for r in rows + [headers])\n",
    "    col3 = max(len(r[2]) for r in rows + [headers])\n",
    "    total_w = col1 + col2 + col3 + 6\n",
    "    border = '=' * total_w\n",
    "    print(f\"\\n{border}\")\n",
    "    print(f\"{headers[0].ljust(col1)} | {headers[1].rjust(col2)} | {headers[2].rjust(col3)}\")\n",
    "    print(border)\n",
    "    for name, cnt, tf in rows:\n",
    "        print(f\"{name.ljust(col1)} | {cnt.rjust(col2)} | {tf.rjust(col3)}\")\n",
    "    print(border + \"\\n\")\n",
    "\n",
    "\n",
    "def prepare_dataset(cfg: ExperimentConfig):\n",
    "    dc = cfg.data\n",
    "    train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "        dc.data_dir,\n",
    "        labels='inferred',\n",
    "        label_mode='binary',\n",
    "        batch_size=dc.batch_size,\n",
    "        image_size=(dc.img_size, dc.img_size),\n",
    "        validation_split=dc.validation_split,\n",
    "        subset='training',\n",
    "        seed=dc.seed\n",
    "    )\n",
    "    val_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "        dc.data_dir,\n",
    "        labels='inferred',\n",
    "        label_mode='binary',\n",
    "        batch_size=dc.batch_size,\n",
    "        image_size=(dc.img_size, dc.img_size),\n",
    "        validation_split=dc.validation_split,\n",
    "        subset='validation',\n",
    "        seed=dc.seed\n",
    "    )\n",
    "    return train_ds.prefetch(tf.data.AUTOTUNE), val_ds.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "\n",
    "def build_model(cfg: ExperimentConfig):\n",
    "    mc = cfg.model\n",
    "    img_size = cfg.data.img_size\n",
    "    base = getattr(keras.applications, mc.backbone)(\n",
    "        include_top=False,\n",
    "        weights='imagenet' if mc.pretrained else None,\n",
    "        input_shape=(img_size, img_size, 3)\n",
    "    )\n",
    "    block_nums = {int(m.group(1)) for layer in base.layers\n",
    "                  if (m := re.match(r\"^block(\\d+)[a-z]?_\", layer.name))}\n",
    "    unique_blocks = sorted(block_nums)\n",
    "    if mc.blocks_to_unfreeze:\n",
    "        target_blocks = mc.blocks_to_unfreeze\n",
    "    else:\n",
    "        target_blocks = unique_blocks[-mc.unfreeze_blocks:]\n",
    "    for layer in base.layers:\n",
    "        m = re.match(r\"^block(\\d+)[a-z]?_\", layer.name)\n",
    "        freeze = not (m and int(m.group(1)) in target_blocks and not isinstance(layer, layers.BatchNormalization))\n",
    "        layer.trainable = not freeze\n",
    "    inp = keras.Input((img_size, img_size, 3))\n",
    "    x = base(inp, training=False)\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "    out = layers.Dense(1, activation='sigmoid')(x)\n",
    "    model = keras.Model(inp, out)\n",
    "    return model, base\n",
    "\n",
    "\n",
    "def compile_and_train(model: keras.Model, train_ds, val_ds, cfg: ExperimentConfig):\n",
    "    tc = cfg.training\n",
    "    cb = cfg.callbacks\n",
    "    opt = getattr(keras.optimizers, tc.optimizer.capitalize())(learning_rate=tc.learning_rate)\n",
    "    model.compile(optimizer=opt, loss=tc.loss, metrics=tc.metrics)\n",
    "    callbacks = []\n",
    "    if cb.tensorboard_logdir:\n",
    "        callbacks.append(keras.callbacks.TensorBoard(log_dir=cb.tensorboard_logdir))\n",
    "    if cb.checkpoint_dir:\n",
    "        p = Path(cb.checkpoint_dir)\n",
    "        p.mkdir(parents=True, exist_ok=True)\n",
    "        callbacks.append(\n",
    "            keras.callbacks.ModelCheckpoint(\n",
    "                filepath=str(p/'ckpt_{epoch}.keras'), save_best_only=True,\n",
    "                monitor='val_loss')\n",
    "        )\n",
    "    return model.fit(train_ds, validation_data=val_ds, epochs=tc.epochs, callbacks=callbacks)\n",
    "\n",
    "\n",
    "def plot_history(history):\n",
    "    hist = history.history\n",
    "    epochs = range(1, len(hist['loss']) + 1)\n",
    "    plt.figure()\n",
    "    plt.plot(epochs, hist['loss'], label='Training Loss')\n",
    "    plt.plot(epochs, hist['val_loss'], label='Validation Loss')\n",
    "    plt.title('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    if 'accuracy' in hist:\n",
    "        plt.figure()\n",
    "        plt.plot(epochs, hist['accuracy'], label='Training Accuracy')\n",
    "        plt.plot(epochs, hist['val_accuracy'], label='Validation Accuracy')\n",
    "        plt.title('Accuracy')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "def run_experiment(**overrides):\n",
    "    cfg = ExperimentConfig()\n",
    "    for section, params in overrides.items():\n",
    "        if hasattr(cfg, section) and isinstance(params, dict):\n",
    "            old = getattr(cfg, section)\n",
    "            new = replace(old, **params)\n",
    "            cfg = replace(cfg, **{section: new})\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown section '{section}' or invalid params\")\n",
    "    cfg_dict = {}\n",
    "    flat = asdict(cfg)\n",
    "    for sec, sec_vals in flat.items():\n",
    "        for k, v in sec_vals.items():\n",
    "            cfg_dict[f\"{sec}.{k}\"] = v\n",
    "    print_block(\"Experiment Config\", cfg_dict)\n",
    "    train_ds, val_ds = prepare_dataset(cfg)\n",
    "    model, base = build_model(cfg)\n",
    "    summarize_model(base)\n",
    "    history = compile_and_train(model, train_ds, val_ds, cfg)\n",
    "    plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "14e1902b-b209-4fb3-a3ac-4a4a17cffdcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b10a343-c263-45cb-a566-e3cb65c62d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAMPLE EXPERIMENT: Do not use as an actual experiment\n",
    "exp['SAMPLE'] = run_experiment(\n",
    "    training={\n",
    "        'metrics': ['auc', 'accuracy', 'precision', 'recall']\n",
    "    }\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
